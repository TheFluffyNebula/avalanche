1. warm initialization -- preserve previous v_star instead of initializing to zero
2. no vanilla python for loop in gradient descent
3. instead of two matrix vector products (precomputation of G * transpose(G))
4. compute minimum eigenvalue of Gg matrix and set that to the learning rate (Truss region)
-- in the same vein, dynamic learning rate
